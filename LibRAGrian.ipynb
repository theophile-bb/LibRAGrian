{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LibRAGrian : Building a RAG pipeline with Chonkie Chunker & Faiss"
      ],
      "metadata": {
        "id": "BxqodLfAPeIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs"
      ],
      "metadata": {
        "id": "9i__9f47QJEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "!git clone https://github.com/theophile-bb/LibRAGrian.git\n",
        "%cd LibRAGrian"
      ],
      "metadata": {
        "id": "cwRQlOalQLYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "9cQ72Mn8ydm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"src\")\n",
        "\n",
        "# Utils\n",
        "from utils import *"
      ],
      "metadata": {
        "id": "RWK8ipvVgaQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we have to get the book dataset. We'll retrieve it using the Hugginface and start the processing.\n",
        "\n",
        "Dataset : https://huggingface.co/datasets/stas/gutenberg-100"
      ],
      "metadata": {
        "id": "mEYmC3eMPr0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "df = pd.read_csv(\"hf://datasets/stas/gutenberg-100/books-100.csv\")"
      ],
      "metadata": {
        "id": "HldTOcE99kxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "AdMRgAku9neI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "kJ18kKGi5aoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text processing and cleaning"
      ],
      "metadata": {
        "id": "Nr6nnSHj5cVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We keep the relevant columns and remove the book duplicates."
      ],
      "metadata": {
        "id": "Vk8SC0eFtG4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "book_df = df[['title','author','text']]\n",
        "book_df = book_df.drop_duplicates(subset=[\"title\",\"author\"])"
      ],
      "metadata": {
        "id": "njewkSXlEByg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book_df"
      ],
      "metadata": {
        "id": "su9yJXgkELrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then have to clean and strip the unnecessary parts of the texts.\n",
        "\n",
        "The cleaning part is as follow :\n",
        "- Decode the UTF-8 BOM text bytes to show punctuation.\n",
        "- Keep only the main text related to the book that is located between the *START OF THE PROJECT GUTENBERG EBOOK.* and *END OF THE PROJECT GUTENBERG EBOOK.* tags.\n",
        "- Remove ther unnecessary line jumps (*\\n*) to improve clarity.\n",
        "- Strip once again the redundant noise located at the beginning of the text. It includes : everything before the main title of the book, illustration description, translation and diffusion credits."
      ],
      "metadata": {
        "id": "3Ix171QGtOTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the processing to the whole text corpus."
      ],
      "metadata": {
        "id": "1rUMopR3wGxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "book_df[\"cleaned_text\"] = book_df.apply(lambda row: clean_book(row[\"text\"], row[\"title\"]),axis=1)"
      ],
      "metadata": {
        "id": "nalwEgn4aLzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_
